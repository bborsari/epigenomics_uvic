#*******
# 1. prepare folders to store
# different results
mkdir -p fastq.files bigBed.files bigWig.files bed.files tsv.files pipeline.run downstream.analyses
#*******

#*******
# 2. download metadata for 
# experiments with Assay type = "DNA binding"
# and Biosample term name: "stomach" AND "sigmoid colon" 
../bin/download.metadata.sh "https://www.encodeproject.org/metadata/?type=Experiment&replicates.library.biosample.donor.uuid=d370683e-81e7-473f-8475-7716d027849b&status=released&assembly=GRCh38&biosample_ontology.term_name=sigmoid+colon&biosample_ontology.term_name=stomach&assay_slims=DNA+binding" 
#*******

#*******
# 3. check how many fastq files are available for the experiment of interest (H3K4me3, stomach)
grep -F H3K4me3 metadata.tsv | grep -F stomach | awk 'BEGIN{FS="\t"}$2=="fastq"{n++}END{print n}' 
#*******

#*******
# 4. retrieve fastq files' ids for IP
# please note that there are no control files annotated,
# (audit term "Missing_controlled_by") 
# so we have to retrieve them manually
# from the experiment's page on the ENCODE portal 
# (https://www.encodeproject.org/experiments/ENCSR063HOI/)
grep -F H3K4me3 metadata.tsv | grep -F stomach | awk 'BEGIN{FS=OFS="\t"; print "file_id\ttissue\ttarget\ttype"}$2=="fastq"{print $1, $7, $19, "IP"}' > fastq.ids.txt
#*******

#*******
# 5. manually add fastq files' ids for control experiment
for filename in ENCFF752FJE ENCFF425YBL ENCFF233IBR ENCFF362JMQ; do echo -e "$filename\tstomach\tH3K4me3-human\tcontrol" >> fastq.ids.txt; done
#*******

#*******
# 6. download fastq files (for both IP and control)
# inside fastq.files folder
cut -f1 fastq.ids.txt | tail -n+2 | while read filename; do wget -P fastq.files "https://www.encodeproject.org/files/$filename/@@download/$filename.fastq.gz"; done
#*******

#*******
# 7. check md5sum of the downloaded fastq files

# 7.1. retrieve md5sum of fastq files from the metadata
../bin/selectRows.sh <(cut -f1 fastq.ids.txt) metadata.tsv | cut -f1,41 > fastq.files/md5sum.txt

# 7.2. compute md5sum on the downloaded fastq files  
cat fastq.files/md5sum.txt | while read filename original_md5sum; do md5sum fastq.files/"$filename".fastq.gz | awk -v filename="$filename" -v original_md5sum="$original_md5sum" 'BEGIN{FS=" "; OFS="\t"}{print filename, original_md5sum, $1}'; done > tmp; mv tmp fastq.files/md5sum.txt

# 7.3. make sure there are no files 
# for which original and computed 
# md5sum values differ
awk '$2!=$3' fastq.files/md5sum.txt 
#*******

#*******
# 8. prepare folder to store the genome database
mkdir pipeline.run/genome
#*******

#*******
# 9. download genome database
# please note: update path to chip-seq-pipeline2
# according to where you installed it
bash ~/chip-seq-pipeline2/scripts/download_genome_data.sh hg38 pipeline.run/genome
#*******

#*******
# 10. check run-type (paired- or single-end)
# of IP & control experiments
# please note: you will need to specify it in the input json file
../bin/selectRows.sh <(cut -f1 fastq.ids.txt) metadata.tsv | cut -f35
#*******

#*******
# 11. check fragment length of IP & control experiments
# please note: you may need to provide it in 
# the input json file in case
# the pipeline fails to compute it
../bin/selectRows.sh <(cut -f1 fastq.ids.txt) metadata.tsv | cut -f33
#*******

#*******
# 12. run pipeline
cd pipeline.run
caper run ~/chip-seq-pipeline2/chip.wdl -i input.histone.json --docker > log.txt
cd ..
#*******

#*******
# 13. retrieve ids for bigBed peak calling files
grep -F H3K4me3 metadata.tsv | awk 'BEGIN{FS=OFS="\t"; print "file_id\ttissue\ttarget" } $2=="bigBed_narrowPeak" && $3=="stable_peaks" && $44!="hg19" {print $1, $7, $19}' > bigBed.peaks.ids.txt
#*******

#*******
# 14. retrieve ids for bigWig fold-change files
grep -F H3K4me3 metadata.tsv | awk 'BEGIN{FS=OFS="\t"; print "file_id\ttissue\ttarget" } $2=="bigWig" && $3=="fold_change_over_control" && $44!="hg19" {print $1, $7, $19}' > bigWig.FC.ids.txt
#*******

#*******
# 15. download bigBed and bigWig files
# inside the corresponding folders

# 15.1 bigBed files
cut -f1 bigBed.peaks.ids.txt | tail -n+2 | while read filename; do wget -P bigBed.files "https://www.encodeproject.org/files/$filename/@@download/$filename.bigBed"; done

# 15.2 bigWig files
cut -f1 bigWig.FC.ids.txt | tail -n+2 | while read filename; do wget -P bigWig.files "https://www.encodeproject.org/files/$filename/@@download/$filename.bigWig"; done
#*******

#*******
# 16. check md5sum of the downloaded 
# bigBed and bigWig files

for file_type in bigBed bigWig; do
	
	# 16.1. retrieve md5sum of the files from the metadata
	../bin/selectRows.sh <(cut -f1 "$file_type".*.ids.txt) metadata.tsv | cut -f1,41 > "$file_type".files/md5sum.txt
	
	# 16.2. compute md5sum on the downloaded files  
	cat "$file_type".files/md5sum.txt | while read filename original_md5sum; do md5sum "$file_type".files/"$filename"."$file_type" | awk -v filename="$filename" -v original_md5sum="$original_md5sum" 'BEGIN{FS=" "; OFS="\t"}{print filename, original_md5sum, $1}'; done > tmp; mv tmp "$file_type".files/md5sum.txt
	
	# 16.3. make sure there are no files for which original and computed md5sum values differ
	awk '$2!=$3' "$file_type".files/md5sum.txt

done
#******

#******
# 17. retrieve gtf file for gencode v24
# manual download from: https://www.encodeproject.org/references/ENCSR884DHJ/
mv Downloads/gencode.v24.primary_assembly.annotation.gtf.gz .
#******

#******
# 18. uncompress gtf.gz file
gunzip gencode.v24.primary_assembly.annotation.gtf.gz
#******

#******
# 19. prepare bed file with gene body
# coordinates for protein coding genes
awk '$3=="gene"' gencode.v24.primary_assembly.annotation.gtf | grep -F "protein_coding" | cut -d ";" -f1 | awk 'BEGIN{OFS="\t"}{print $1, $4, $5, $10, 0, $7, $10}' | sed 's/\"//g' | awk 'BEGIN{FS=OFS="\t"}$1!="chrM"{$2=($2-1); print $0}' > bed.files/gencode.v24.protein.coding.gene.body.bed
#******

#******
# 20. retrieve (manually) ids for total RNA-Seq
# gene quantification .tsv files
echo -e "file_id\ttissue\nENCFF268RWA\tsigmoid_colon\nENCFF918KPC\tstomach" > tsv.totalRNASeq.ids.txt
#******

#******
# 21. download .tsv files
cut -f1 tsv.totalRNASeq.ids.txt | tail -n+2 | while read filename; do wget -P tsv.files "https://www.encodeproject.org/files/$filename/@@download/$filename.tsv"; done
#******

#******
# 22. keep only protein-coding genes from tsv files
cut -f1 tsv.totalRNASeq.ids.txt | tail -n+2 | while read filename; do ../bin/selectRows.sh <(cut -f4 bed.files/gencode.v24.protein.coding.gene.body.bed) <(cut -f1,6 tsv.files/"$filename".tsv) > tmp; mv tmp tsv.files/"$filename".tsv; done
#******

#******
# 23. select the 1000 most expressed genes in each of the two tissues
tail -n+2 tsv.totalRNASeq.ids.txt | while read filename tissue; do sort -k2,2gr tsv.files/"$filename".tsv | head -1000 | cut -f1 > tsv.files/"$tissue".1000.most.expressed.genes.txt; done
#******

#******
# 24. select the 1000 least expressed genes in each of the two tissues
tail -n+2 tsv.totalRNASeq.ids.txt | while read filename tissue; do sort -k2,2gr tsv.files/"$filename".tsv | tail -1000 | cut -f1 > tsv.files/"$tissue".1000.least.expressed.genes.txt; done
#******

#******
# 25. prepare bed files for 1000 
# most and least expressed
# genes in the 2 tissues

# 25.1. 1000 least expressed
for tissue in stomach sigmoid_colon; do ../bin/selectRows.sh tsv.files/"$tissue".1000.least.expressed.genes.txt <(awk 'BEGIN{FS=OFS="\t"}{print $4, $0}' bed.files/gencode.v24.protein.coding.gene.body.bed) | cut -f2- > bed.files/"$tissue".1000.least.expressed.genes.bed; done

# 25.2. 1000 most expressed
for tissue in stomach sigmoid_colon; do ../bin/selectRows.sh tsv.files/"$tissue".1000.most.expressed.genes.txt <(awk 'BEGIN{FS=OFS="\t"}{print $4, $0}' bed.files/gencode.v24.protein.coding.gene.body.bed) | cut -f2- > bed.files/"$tissue".1000.most.expressed.genes.bed; done
#******

#******
# 26. prepare folder to store
# results for aggregation analysis
mkdir downstream.analyses/aggregation.plot
#******

#******
# 27. run bwtool aggregate

# 27.1. - stomach 
# 27.1.1. - 1000 most expressed genes
bwtool aggregate 2000:2000 -starts -keep-bed bed.files/stomach.1000.most.expressed.genes.bed bigWig.files/ENCFF373VDB.bigWig downstream.analyses/aggregation.plot/stomach.1000.most.expressed.genes.aggregate.tsv
# 27.1.2. - 1000 least expressed genes
bwtool aggregate 2000:2000 -starts -keep-bed bed.files/stomach.1000.least.expressed.genes.bed bigWig.files/ENCFF373VDB.bigWig downstream.analyses/aggregation.plot/stomach.1000.least.expressed.genes.aggregate.tsv

# 27.2. - sigmoid_colon
# 27.2.1. - 1000 most expressed genes
bwtool aggregate 2000:2000 -starts -keep-bed bed.files/sigmoid_colon.1000.most.expressed.genes.bed bigWig.files/ENCFF647ISC.bigWig downstream.analyses/aggregation.plot/sigmoid_colon.1000.most.expressed.genes.aggregate.tsv
# 27.2.2. - 1000 least expressed genes
bwtool aggregate 2000:2000 -starts -keep-bed bed.files/sigmoid_colon.1000.least.expressed.genes.bed bigWig.files/ENCFF647ISC.bigWig downstream.analyses/aggregation.plot/sigmoid_colon.1000.least.expressed.genes.aggregate.tsv
#******

#******
# 28. make aggregation plot
for tissue in stomach sigmoid_colon; do Rscript ../bin/aggregation.plot.R --most downstream.analyses/aggregation.plot/"$tissue".1000.most.expressed.genes.aggregate.tsv --least downstream.analyses/aggregation.plot/"$tissue".1000.least.expressed.genes.aggregate.tsv --tissue "$tissue" --output downstream.analyses/aggregation.plot/aggregation.plot."$tissue".pdf; done
#******

#******
# 29. prepare folder to 
# store results from building ChIP-seq matrix
mkdir -p downstream.analyses/build.matrix/bwtool
#******

#******
# 30. prepare bed file
# with coordinates of 
# non-redundant TSSs (all genes)
../bin/non.redundant.TSS.sh gencode.v24.primary_assembly.annotation.gtf > bed.files/gencode.v24.all.genes.non.redundant.TSS.bed
#******

#******
# 31. prepare bed file 
# with coordinates of  
# non-redundant TSSs (protein coding genes)
../bin/selectRows.sh <(cut -f7 bed.files/gencode.v24.protein.coding.gene.body.bed) <(awk 'BEGIN{FS=OFS="\t"}{print $7, $0}' bed.files/gencode.v24.all.genes.non.redundant.TSS.bed) | cut -f2- > bed.files/gencode.v24.protein.coding.non.redundant.TSS.bed
#******

#******
# 32. prepare list of bigWig 
# FC files to be used 
# to build the matrix
my_pwd=$(pwd); tail -n+2 bigWig.FC.ids.txt | cut -f1 | awk -v pwd="$my_pwd" '{print pwd"/bigWig.files/"$1".bigWig"}' > downstream.analyses/build.matrix/FC.bw.txt
#******

#******
# 33. convert bigBed files 
# to bed files
tail -n+2 bigBed.peaks.ids.txt | cut -f1 | while read filename; do bigBedToBed bigBed.files/"$filename".bigBed bed.files/"$filename".bed; done
#******

#******
# 34. prepare list of bed
# peak files to be used
# to build the matrix
my_pwd=$(pwd); tail -n+2 bigBed.peaks.ids.txt | cut -f1 | awk -v pwd="$my_pwd" '{print pwd"/bed.files/"$1".bed"}' > downstream.analyses/build.matrix/peaks.bed.txt
#******

#******
# 35. get H3K4me3 matrix
my_pwd=$(pwd); ../bin/get.matrix.chipseq.sh --bw downstream.analyses/build.matrix/FC.bw.txt --bedfile "$my_pwd"/bed.files/gencode.v24.protein.coding.non.redundant.TSS.bed --target H3K4me3 --outFolder downstream.analyses/build.matrix/bwtool --outFile downstream.analyses/build.matrix/H3K4me3.matrix.tsv --signal median --peaks downstream.analyses/build.matrix/peaks.bed.txt --keep yes
#******

#******
# 36. update header
awk 'BEGIN{FS=OFS="\t"}{if (NR==1) {$1="sigmoid_colon"; $2="stomach"; print $0} else {print $0}}' downstream.analyses/build.matrix/H3K4me3.matrix.tsv > downstream.analyses/build.matrix/tmp; mv downstream.analyses/build.matrix/tmp downstream.analyses/build.matrix/H3K4me3.matrix.tsv
#******

#******
# 37. before generating
# the expression matrix,
# check that the row order is the same
# in the 2 tsv files
diff <(cut -f1 tsv.files/ENCFF268RWA.tsv) <(cut -f1 tsv.files/ENCFF918KPC.tsv)
#******

#******
# 38. get expression matrix
# 1st column = sigmoid_colon
# 2nd column = stomach
paste tsv.files/ENCFF268RWA.tsv <(cut -f2 tsv.files/ENCFF918KPC.tsv) | awk 'BEGIN{FS=OFS="\t"; print "sigmoid_colon", "stomach"}{print}' > downstream.analyses/build.matrix/expression.matrix.tsv
#******

#******
# 39. check that the row order
# is the same for expression
# and H3K4me3 matrices
diff <(cut -f1 downstream.analyses/build.matrix/H3K4me3.matrix.tsv) <(cut -f1 downstream.analyses/build.matrix/expression.matrix.tsv)
#******

#******
# 40. prepare folder to
# store results of correlation analyses
mkdir downstream.analyses/scatterplot.correlation
#******

#******
# 41. scatterplot with
# Pearson & Spearman's correlation
# coefficient between
# expression and H3K4me3 values
for tissue in sigmoid_colon stomach; do Rscript ../bin/scatterplot.correlation.R --expression downstream.analyses/build.matrix/expression.matrix.tsv --mark downstream.analyses/build.matrix/H3K4me3.matrix.tsv --tissue "$tissue" --output downstream.analyses/scatterplot.correlation/scatterplot.correlation."$tissue".pdf; done
#******

#******
# 42. prepare folder to store
# analysis of genes with peaks
mkdir downstream.analyses/peaks.analysis
#******

#******
# 43. retrieve protein-coding 
# genes with peaks of H3K4me3
# in each tissue
cut -f-2 bigBed.peaks.ids.txt | tail -n+2 | while read filename tissue; do bedtools intersect -a bed.files/gencode.v24.protein.coding.non.redundant.TSS.bed -b bed.files/"$filename".bed -u | cut -f7 | sort -u > downstream.analyses/peaks.analysis/genes.with.peaks."$tissue".txt; done
#******

#******
# 44. number of protein-coding 
# genes with peaks of H3K4me3
# in each tissue
wc -l downstream.analyses/peaks.analysis/genes.with.peaks*
#******

#******
# 45. retrieve protein-coding 
# genes with peaks of H3K4me3
# in BOTH tissues
../bin/selectRows.sh downstream.analyses/peaks.analysis/genes.with.peaks.stomach.txt downstream.analyses/peaks.analysis/genes.with.peaks.sigmoid_colon.txt | cut -d "." -f1 > downstream.analyses/peaks.analysis/genes.marked.both.tissues.txt
#******

#******
# 46. retrieve protein-coding
# genes with peaks of H3K4me3
# in only one tissue

# 46.1. genes with stomach-specific peaks
../bin/discardRows.sh downstream.analyses/peaks.analysis/genes.with.peaks.sigmoid_colon.txt downstream.analyses/peaks.analysis/genes.with.peaks.stomach.txt | cut -d "." -f1 > downstream.analyses/peaks.analysis/genes.with.stomach.specific.peaks.txt

# 46.2. genes with sigmoid_colon-specific peaks
../bin/discardRows.sh downstream.analyses/peaks.analysis/genes.with.peaks.stomach.txt downstream.analyses/peaks.analysis/genes.with.peaks.sigmoid_colon.txt | cut -d "." -f1 > downstream.analyses/peaks.analysis/genes.with.sigmoid_colon.specific.peaks.txt
#******

#******
# 47. retrieve protein-coding genes
# without peaks of H3K4me3
# in both tissues
../bin/discardRows.sh <(cat downstream.analyses/peaks.analysis/genes.marked.both.tissues.txt downstream.analyses/peaks.analysis/genes.with.stomach.specific.peaks.txt downstream.analyses/peaks.analysis/genes.with.sigmoid_colon.specific.peaks.txt) <(cut -f7 bed.files/gencode.v24.protein.coding.gene.body.bed | cut -d "." -f1) > downstream.analyses/peaks.analysis/genes.not.marked.txt
#******

#******
# 48. define as universe for GO
# analysis the set of protein-coding genes
cut -f7 bed.files/gencode.v24.protein.coding.gene.body.bed | cut -d "." -f1 > downstream.analyses/peaks.analysis/universe.genes.txt
#******

#******
# 49. perform GO analysis
# with Metascape: http://metascape.org/gp/index.html#/main/step1 
# 49.1.: genes with tissue-specific marking
# 49.2.: genes with no peaks in any tissue
#******

#******
# 50. compare expression
# between the different sets of genes
# (a. marked in both tissues,
# b. marked in stomach
# c. marked in sigmoid_colon,
# d. not marked in any tissue)
Rscript ../bin/boxplot.expression.R --expression downstream.analyses/build.matrix/expression.matrix.tsv --marked_both_tissues downstream.analyses/peaks.analysis/genes.marked.both.tissues.txt --stomach_specific downstream.analyses/peaks.analysis/genes.with.stomach.specific.peaks.txt --sigmoid_colon_specific downstream.analyses/peaks.analysis/genes.with.sigmoid_colon.specific.peaks.txt --not_marked downstream.analyses/peaks.analysis/genes.not.marked.txt --output downstream.analyses/peaks.analysis/boxplot.expression.pdf
#******

#******
# 51. instance of json file
# to run pipeline on TFs

#******

#******
# 52. retrieve ids for 
# bigBed peak calling files
# for POLR2A
grep -F POLR2A-human metadata.tsv | awk 'BEGIN{FS=OFS="\t"} $2=="bigBed_narrowPeak" && $3=="pseudoreplicated_IDR_thresholded_peaks" && $44!="hg19" {print $1, $7, $19}' | sort -k2,2 >> bigBed.peaks.ids.txt 
#******

#******
# 53. download bigBed files 
# inside the corresponding folders
awk '$3=="POLR2A-human"{print $1}' bigBed.peaks.ids.txt | while read filename; do wget -P bigBed.files "https://www.encodeproject.org/files/$filename/@@download/$filename.bigBed"; done
#******

#******
# 54. check md5sum of 
# the downloaded bigBed files

# 54.1. retrieve md5sum of the files from the metadata
../bin/selectRows.sh <(awk '$3=="POLR2A-human"{print $1}' bigBed.peaks.ids.txt) metadata.tsv | cut -f1,41 >> bigBed.files/tmp

# 54.2. compute md5sum on the downloaded files
cat bigBed.files/tmp | while read filename original_md5sum; do md5sum bigBed.files/"$filename".bigBed | awk -v filename="$filename" -v original_md5sum="$original_md5sum" 'BEGIN{FS=" "; OFS="\t"}{print filename, original_md5sum, $1}'; done >> bigBed.files/md5sum.txt; rm tmp

# 54.3. make sure there are no files 
# for which original and computed md5sum values differ
awk '$2!=$3' bigBed.files/md5sum.txt
#******

#******
# 55. convert bigBed files 
# to bed files
awk '$3=="POLR2A-human"{print $1}' bigBed.peaks.ids.txt | while read filename; do bigBedToBed bigBed.files/"$filename".bigBed bed.files/"$filename".bed; done
#******
